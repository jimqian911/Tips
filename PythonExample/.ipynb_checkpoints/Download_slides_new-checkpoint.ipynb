{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [here](https://null-byte.wonderhowto.com/how-to/download-all-pdfs-webpage-with-python-script-0163031/)\n",
    "* [here](https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urllib\n",
    "import urllib3\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllibR\n",
    "from requests import get\n",
    "from time import sleep, time\n",
    "import re\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to download (bypass using firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def obtain_bs4soup(url, bypass):\n",
    "    try:\n",
    "        os.mkdir(download_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "    \n",
    "    ## Choose whether to bypass through firefox\n",
    "    if bypass == True:\n",
    "        request0 = urllibR.Request(url=url, headers=headers)\n",
    "        request = urllibR.urlopen(request0)\n",
    "    else:\n",
    "        request = urllibR.urlopen(url)\n",
    "        \n",
    "    soup = BeautifulSoup(request.read(), \"lxml\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        try:\n",
    "            # get request\n",
    "            response = get(url)\n",
    "            # write to file\n",
    "            file.write(response.content)\n",
    "        except:\n",
    "            print(\"fail\", file_name)\n",
    "#         print(\"wrote\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "url = \"https://imai.princeton.edu/teaching/uG.html\"\n",
    "# url = \"https://projects.iq.harvard.edu/prefresher/math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "download_path = \"/Users/tomoyasasaki/Documents/Materials/Lectures/Imai_handouts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "soup = obtain_bs4soup(url, bypass = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.html\n",
      "index.html\n",
      "syl345.pdf\n",
      "syl345.pdf\n",
      "uG.html\n",
      "introduction.pdf\n",
      "descriptive.pdf\n",
      "probabilityuG.pdf\n",
      "statistics.pdf\n",
      "infosession.pdf\n",
      "Handout1.pdf\n",
      "Precept1.pdf\n",
      "Handout2.pdf\n",
      "Precept2.pdf\n",
      "Handout3.pdf\n",
      "Precept3.pdf\n",
      "Handout4.pdf\n",
      "Precept4.pdf\n",
      "Precept5.pdf\n",
      "Handout6.pdf\n",
      "Precept6.pdf\n",
      "Handout7.pdf\n",
      "Precept7.pdf\n",
      "Handout8.pdf\n",
      "Precept8.pdf\n",
      "Handout9.pdf\n",
      "Precept9.pdf\n",
      "Handout10.pdf\n",
      "Precept10.pdf\n"
     ]
    }
   ],
   "source": [
    "## if the URL for the target file is straight forward, use this\n",
    "## e.g. <a href=\"/path/to/file.pdf\">\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    if os.path.splitext(os.path.basename(tag2))[1] == \".pdf\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".ipynb\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".py\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".tex\" or\\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".zip\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".ppt\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".RData\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".html\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".R\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".txt\":\n",
    "\n",
    "#     if len( os.path.splitext(os.path.basename(tag2))[1]  ) >= 1:\n",
    "        name = os.path.basename(tag2)\n",
    "#         name = os.path.basename(tag2)[:-2]\n",
    "        download(tag2, name)\n",
    "        print(name)\n",
    "        sleep(1)\n",
    "\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics1_2017\n",
      "metrics2_2017\n",
      "metrics3_2017\n",
      "metrics4_2017\n",
      "metrics5_2017\n",
      "metrics6_2017\n"
     ]
    }
   ],
   "source": [
    "## if the URL for the target file is NOT straight forward, use this\n",
    "## e.g. <a href=\"path/to/file.pdf?attredirects=0&amp;d=1\">\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    if \".pdf\" in os.path.splitext(os.path.basename(tag2))[1]:\n",
    "        name = os.path.splitext(os.path.basename(tag2))[0]\n",
    "        download(tag2, name + \".pdf\")\n",
    "        print(name)\n",
    "        sleep(1)\n",
    "\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "#     print(os.path.splitext(os.path.basename(tag['href'])) )\n",
    "#     print(tag2)\n",
    "#     if os.path.splitext(os.path.basename(tag2))[1] == \".pdf\" or os.path.splitext(os.path.basename(tag2))[1] == \".r\"\\\n",
    "    if \".xls\" in os.path.splitext(os.path.basename(tag2))[1]:\n",
    "#         print(tag2)\n",
    "#         name = os.path.basename(tag2)[:-2]\n",
    "        name = os.path.splitext(os.path.basename(tag2))[0]\n",
    "#         download(tag2, name)\n",
    "        download(tag2, name + \".xls\")\n",
    "        print(name)\n",
    "#         if name == \"sig_phrases_det.pdf\":\n",
    "#             pass\n",
    "#         else:\n",
    "#         download(tag2, re.sub(r\"\\?.+\" ,\"\",os.path.basename(tag2) ))\n",
    "#         download(url + name, name)\n",
    "#         sleep(1)\n",
    "#         tmp.append(tag2)\n",
    "\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/?C=N;O=D\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/?C=M;O=A\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/?C=S;O=A\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/?C=D;O=A\n",
      "https://www.stat.washington.edu/people/pdhoff/Book/Data/\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/XY.tumor\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.mathscore\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.pima.full\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.pima.miss\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.reading\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.school.mathscore\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/Y.tumor\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/alldata\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/chapter7.r\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/chapter8.r\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/chapter9.r\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/chapter10.r\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/chapter11.r\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/ids_selectschools\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/mathdat\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/nels_2002\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/readme.txt\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/vostok.1999.temp.dat\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/vostok.icecore.co2.dat\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/y.school1\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/y.school2\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/yX.diabetes.test\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/yX.diabetes.train\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/yX.o2uptake\n",
      "https://www.stat.washington.edu/~pdhoff/Book/Data/data/yX.sparrow\n"
     ]
    }
   ],
   "source": [
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    print(tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
